{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: Path Configuration (Update these to match your Drive/dataset locations)\n",
        "\n",
        "# X-Sub (default, uncomment to train X-Sub)\n",
        "Xsub_train_data = '/content/drive/MyDrive/ntu_normalized/xsub_train_data_joint.npy'\n",
        "Xsub_train_label = '/content/drive/MyDrive/ntu_processed/xsub/train/train_label.pkl'\n",
        "Xsub_val_data = '/content/drive/MyDrive/ntu_normalized/xsub_val_data_joint.npy'\n",
        "Xsub_val_label = '/content/drive/MyDrive/ntu_processed/xsub/val/val_label.pkl'\n",
        "\n",
        "# X-View (use these for X-View)\n",
        "Xview_train_data = '/content/drive/MyDrive/ntu_normalized/xview_train_data_joint.npy'\n",
        "Xview_train_label = '/content/drive/MyDrive/ntu_processed/xview/train/train_label.pkl'\n",
        "Xview_val_data = '/content/drive/MyDrive/ntu_normalized/xview_val_data_joint.npy'\n",
        "Xview_val_label = '/content/drive/MyDrive/ntu_processed/xview/val/val_label.pkl'\n"
      ],
      "metadata": {
        "id": "w4TC-bCW513n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sm4VCOuXqmiQ",
        "outputId": "fb442c01-8b4e-4640-8363-4aa7a296a26a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1: Install Dependencies (A100 optimized)\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install tensorboardX tqdm pyyaml einops\n",
        "!pip install torchinfo\n",
        "!pip install einops"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "eWqlRCSJU7Pp",
        "outputId": "3340ef60-2c2e-46bb-a18f-16eb663e9f29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "INFO: pip is looking at multiple versions of torch to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting torch\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torch-2.7.1%2Bcu118-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (28 kB)\n",
            "Collecting sympy>=1.13.3 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/sympy-1.13.3-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting nvidia-cuda-nvrtc-cu11==11.8.89 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_nvrtc_cu11-11.8.89-py3-none-manylinux1_x86_64.whl (23.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.2/23.2 MB\u001b[0m \u001b[31m78.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.8.89 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_runtime_cu11-11.8.89-py3-none-manylinux1_x86_64.whl (875 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m875.6/875.6 kB\u001b[0m \u001b[31m56.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu11==11.8.87 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_cupti_cu11-11.8.87-py3-none-manylinux1_x86_64.whl (13.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m94.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu11==9.1.0.70 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cudnn_cu11-9.1.0.70-py3-none-manylinux2014_x86_64.whl (663.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m663.9/663.9 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu11==11.11.3.6 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cublas_cu11-11.11.3.6-py3-none-manylinux1_x86_64.whl (417.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m417.9/417.9 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu11==10.9.0.58 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu11==10.3.0.86 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_curand_cu11-10.3.0.86-py3-none-manylinux1_x86_64.whl (58.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.1/58.1 MB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu11==11.4.1.48 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cusolver_cu11-11.4.1.48-py3-none-manylinux1_x86_64.whl (128.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.2/128.2 MB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu11==11.7.5.86 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cusparse_cu11-11.7.5.86-py3-none-manylinux1_x86_64.whl (204.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m204.1/204.1 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu11==2.21.5 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_nccl_cu11-2.21.5-py3-none-manylinux2014_x86_64.whl (147.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 MB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu11==11.8.86 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_nvtx_cu11-11.8.86-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting triton==3.3.1 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/triton-3.3.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.3.1->torch) (75.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n",
            "Collecting torch\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torch-2.6.0%2Bcu118-cp311-cp311-linux_x86_64.whl.metadata (27 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading https://download.pytorch.org/whl/cu118/torch-2.6.0%2Bcu118-cp311-cp311-linux_x86_64.whl (848.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m848.7/848.7 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, nvidia-cusolver-cu11, nvidia-cudnn-cu11, torch\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.6.0+cu124\n",
            "    Uninstalling torch-2.6.0+cu124:\n",
            "      Successfully uninstalled torch-2.6.0+cu124\n",
            "Successfully installed nvidia-cublas-cu11-11.11.3.6 nvidia-cuda-cupti-cu11-11.8.87 nvidia-cuda-nvrtc-cu11-11.8.89 nvidia-cuda-runtime-cu11-11.8.89 nvidia-cudnn-cu11-9.1.0.70 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.3.0.86 nvidia-cusolver-cu11-11.4.1.48 nvidia-cusparse-cu11-11.7.5.86 nvidia-nccl-cu11-2.21.5 nvidia-nvtx-cu11-11.8.86 torch-2.6.0+cu118\n",
            "Collecting tensorboardX\n",
            "  Downloading tensorboardx-2.6.4-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (6.0.2)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (0.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from tensorboardX) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorboardX) (24.2)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.11/dist-packages (from tensorboardX) (5.29.5)\n",
            "Downloading tensorboardx-2.6.4-py3-none-any.whl (87 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.6.4\n",
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
            "Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.8.0\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (0.8.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Imports & Helper Functions**"
      ],
      "metadata": {
        "id": "XkqJ941APPb7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add these at the top of your first cell\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "\n",
        "# Add this after device setup\n",
        "torch.save({'epoch': 0}, '/tmp/current_epoch.pth')  # For augmentation tracking"
      ],
      "metadata": {
        "id": "zozDYyuqsmum"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Setup and Imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import pickle\n",
        "import time\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import os\n",
        "from torchinfo import summary\n",
        "from torch.cuda import amp\n",
        "import einops\n",
        "\n",
        "# Enable TF32 and deterministic mode for A100 GPU\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# Set device\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "# Data paths (update with your paths)\n",
        "train_data = '/content/drive/MyDrive/ntu_normalized/xsub_train_data_joint.npy'\n",
        "val_data   = '/content/drive/MyDrive/ntu_normalized/xsub_val_data_joint.npy'\n",
        "train_label = '/content/drive/MyDrive/ntu_processed/xsub/train/train_label.pkl'\n",
        "val_label   = '/content/drive/MyDrive/ntu_processed/xsub/val/val_label.pkl'\n",
        "\n",
        "# Verify files\n",
        "print(\"Train data:\", os.path.exists(train_data), \"| Val data:\", os.path.exists(val_data))\n",
        "print(\"Train label:\", os.path.exists(train_label), \"| Val label:\", os.path.exists(val_label))\n",
        "\n",
        "# Print dataset info\n",
        "with open(train_label, 'rb') as f:\n",
        "    sample_names, sample_labels = pickle.load(f)\n",
        "print(f\"Train samples: {len(sample_labels)} | Example label: {sample_labels[0]}\")\n",
        "with open(val_label, 'rb') as f:\n",
        "    val_names, val_labels = pickle.load(f)\n",
        "print(f\"Val samples: {len(val_labels)} | Example label: {val_labels[0]}\")\n",
        "\n",
        "sample = np.load(train_data, mmap_mode='r')\n",
        "print(\"Train data shape (N,C,T,V,M):\", sample.shape)\n",
        "print(\"One sample min/max:\", sample[0].min(), sample[0].max())\n",
        "print(\"Unique classes in train:\", len(set(sample_labels)))\n",
        "print(\"Unique classes in val:\", len(set(val_labels)))\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "set_seed(42)\n"
      ],
      "metadata": {
        "id": "1VtVqoctLJuD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "bb2b803e-9670-45b7-9bf7-d31b1c721a89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Train data: True | Val data: True\n",
            "Train label: True | Val label: True\n",
            "Train samples: 33383 | Example label: 16\n",
            "Val samples: 23327 | Example label: 13\n",
            "Train data shape (N,C,T,V,M): (33383, 3, 300, 25, 2)\n",
            "One sample min/max: -1.9420302618199796 0.8381361333212642\n",
            "Unique classes in train: 60\n",
            "Unique classes in val: 60\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Enhanced Feeder Class with Advanced Augmentation**"
      ],
      "metadata": {
        "id": "xks6yyxVVPvl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Enhanced Feeder Class with Advanced Augmentation\n",
        "class Feeder(Dataset):\n",
        "    def __init__(self, data_path, label_path, train=False, debug=False, use_mmap=True):\n",
        "        self.debug = debug\n",
        "        self.data_path = data_path\n",
        "        self.label_path = label_path\n",
        "        self.use_mmap = use_mmap\n",
        "        self.train = train\n",
        "        self.load_data()\n",
        "\n",
        "    def load_data(self):\n",
        "        with open(self.label_path, 'rb') as f:\n",
        "            try:\n",
        "                self.sample_name, self.label = pickle.load(f)\n",
        "            except:\n",
        "                self.sample_name, self.label = pickle.load(f, encoding='latin1')\n",
        "        if self.use_mmap:\n",
        "            self.data = np.load(self.data_path, mmap_mode='r')\n",
        "        else:\n",
        "            self.data = np.load(self.data_path)\n",
        "        if self.debug:\n",
        "            self.label = self.label[:100]\n",
        "            self.data = self.data[:100]\n",
        "            self.sample_name = self.sample_name[:100]\n",
        "\n",
        "    def temporal_crop(self, data, crop_length=300):\n",
        "        T = data.shape[1]\n",
        "        if T > crop_length:\n",
        "            start = np.random.randint(0, T - crop_length)\n",
        "            return data[:, start:start+crop_length, :, :]\n",
        "        return data\n",
        "\n",
        "    def spatial_flip(self, data):\n",
        "        if np.random.rand() > 0.5:\n",
        "            data[0] = -data[0]\n",
        "        return data\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        data = np.array(self.data[idx])\n",
        "        label = int(self.label[idx])\n",
        "\n",
        "        if self.train:\n",
        "            try:\n",
        "                epoch = torch.load('/tmp/current_epoch.pth')['epoch']\n",
        "            except:\n",
        "                epoch = 0\n",
        "\n",
        "            aug_prob = min(0.8, 0.4 + epoch*0.03)  # Dynamic probability\n",
        "\n",
        "            if random.random() < aug_prob:\n",
        "                data = self.temporal_crop(data)\n",
        "            if random.random() < aug_prob:\n",
        "                data = self.spatial_flip(data)\n",
        "\n",
        "        return data, label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.label)"
      ],
      "metadata": {
        "id": "mD8NGTbUVNDc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Enhanced AGCN Model with Dropout**"
      ],
      "metadata": {
        "id": "0gjM9-BeVXXY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Enhanced AGCN Model with Dropout\n",
        "import math\n",
        "\n",
        "def conv_branch_init(conv, branches):\n",
        "    weight = conv.weight\n",
        "    n = weight.size(0)\n",
        "    k1 = weight.size(1)\n",
        "    k2 = weight.size(2)\n",
        "    nn.init.normal_(weight, 0, math.sqrt(2. / (n * k1 * k2 * branches)))\n",
        "    nn.init.constant_(conv.bias, 0)\n",
        "\n",
        "def conv_init(conv):\n",
        "    nn.init.kaiming_normal_(conv.weight, mode='fan_out')\n",
        "    nn.init.constant_(conv.bias, 0)\n",
        "\n",
        "def bn_init(bn, scale):\n",
        "    nn.init.constant_(bn.weight, scale)\n",
        "    nn.init.constant_(bn.bias, 0)\n",
        "\n",
        "class unit_tcn(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=9, stride=1):\n",
        "        super(unit_tcn, self).__init__()\n",
        "        pad = int((kernel_size - 1) / 2)\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=(kernel_size, 1), padding=(pad, 0),\n",
        "                              stride=(stride, 1))\n",
        "        self.bn = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        conv_init(self.conv)\n",
        "        bn_init(self.bn, 1)\n",
        "    def forward(self, x):\n",
        "        x = self.bn(self.conv(x))\n",
        "        return x\n",
        "\n",
        "class unit_gcn(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, A, coff_embedding=4, num_subset=3):\n",
        "        super(unit_gcn, self).__init__()\n",
        "        inter_channels = out_channels // coff_embedding\n",
        "        self.inter_c = inter_channels\n",
        "        self.PA = nn.Parameter(torch.from_numpy(A.astype(np.float32)))\n",
        "        nn.init.constant_(self.PA, 1e-6)\n",
        "        self.A = torch.tensor(A.astype(np.float32), requires_grad=False)\n",
        "        self.num_subset = num_subset\n",
        "        self.conv_a = nn.ModuleList()\n",
        "        self.conv_b = nn.ModuleList()\n",
        "        self.conv_d = nn.ModuleList()\n",
        "        for i in range(self.num_subset):\n",
        "            self.conv_a.append(nn.Conv2d(in_channels, inter_channels, 1))\n",
        "            self.conv_b.append(nn.Conv2d(in_channels, inter_channels, 1))\n",
        "            self.conv_d.append(nn.Conv2d(in_channels, out_channels, 1))\n",
        "        if in_channels != out_channels:\n",
        "            self.down = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, 1),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "        else:\n",
        "            self.down = lambda x: x\n",
        "        self.bn = nn.BatchNorm2d(out_channels)\n",
        "        self.soft = nn.Softmax(-2)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                conv_init(m)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                bn_init(m, 1)\n",
        "        bn_init(self.bn, 1e-6)\n",
        "        for i in range(self.num_subset):\n",
        "            conv_branch_init(self.conv_d[i], self.num_subset)\n",
        "    def forward(self, x):\n",
        "        N, C, T, V = x.size()\n",
        "        A = self.A.to(x.device) + self.PA\n",
        "        y = None\n",
        "        for i in range(self.num_subset):\n",
        "            A1 = self.conv_a[i](x).permute(0, 3, 1, 2).contiguous().view(N, V, self.inter_c * T)\n",
        "            A2 = self.conv_b[i](x).view(N, self.inter_c * T, V)\n",
        "            temp = torch.matmul(A1, A2) / (A1.size(-1) + 1e-8)\n",
        "            temp = torch.clamp(temp, -50, 50)\n",
        "            A1 = self.soft(temp)\n",
        "            A1 = A1 + A[i]\n",
        "            A2 = x.view(N, C * T, V)\n",
        "            z = self.conv_d[i](torch.matmul(A2, A1).view(N, C, T, V))\n",
        "            y = z + y if y is not None else z\n",
        "        y = self.bn(y)\n",
        "        y += self.down(x)\n",
        "        return self.relu(y)\n",
        "\n",
        "class TCN_GCN_unit(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, A, stride=1, residual=True):\n",
        "        super(TCN_GCN_unit, self).__init__()\n",
        "        self.gcn1 = unit_gcn(in_channels, out_channels, A)\n",
        "        self.tcn1 = unit_tcn(out_channels, out_channels, stride=stride)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        if not residual:\n",
        "            self.residual = lambda x: 0\n",
        "        elif (in_channels == out_channels) and (stride == 1):\n",
        "            self.residual = lambda x: x\n",
        "        else:\n",
        "            self.residual = unit_tcn(in_channels, out_channels, kernel_size=1, stride=stride)\n",
        "    def forward(self, x):\n",
        "        x = self.tcn1(self.gcn1(x)) + self.residual(x)\n",
        "        return self.relu(x)\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, num_class=60, num_point=25, num_person=2, graph=None, graph_args=dict(), in_channels=3):\n",
        "        super(Model, self).__init__()\n",
        "        class Graph:\n",
        "            def __init__(self, labeling_mode='spatial'):\n",
        "                self.num_node = 25\n",
        "                self.self_link = [(i, i) for i in range(self.num_node)]\n",
        "                inward_ori_index = [\n",
        "                    (1, 2), (2, 21), (3, 21), (4, 3), (5, 21), (6, 5), (7, 6),\n",
        "                    (8, 7), (9, 21), (10, 9), (11, 10), (12, 11), (13, 1),\n",
        "                    (14, 13), (15, 14), (16, 15), (17, 1), (18, 17), (19, 18),\n",
        "                    (20, 19), (22, 23), (23, 8), (24, 25), (25, 12)\n",
        "                ]\n",
        "                inward = [(i - 1, j - 1) for (i, j) in inward_ori_index]\n",
        "                outward = [(j, i) for (i, j) in inward]\n",
        "                neighbor = inward + outward\n",
        "                def edge2mat(link, num_node):\n",
        "                    A = np.zeros((num_node, num_node))\n",
        "                    for i, j in link:\n",
        "                        A[j, i] = 1\n",
        "                    return A\n",
        "                def normalize_digraph(A):\n",
        "                    Dl = np.sum(A, 0)\n",
        "                    h, w = A.shape\n",
        "                    Dn = np.zeros((w, w))\n",
        "                    for i in range(w):\n",
        "                        if Dl[i] > 0:\n",
        "                            Dn[i, i] = Dl[i] ** (-1)\n",
        "                    AD = np.dot(A, Dn)\n",
        "                    return AD\n",
        "                def get_spatial_graph(num_node, self_link, inward, outward):\n",
        "                    I = edge2mat(self_link, num_node)\n",
        "                    In = normalize_digraph(edge2mat(inward, num_node))\n",
        "                    Out = normalize_digraph(edge2mat(outward, num_node))\n",
        "                    A = np.stack((I, In, Out))\n",
        "                    return A\n",
        "                self.A = get_spatial_graph(self.num_node, self.self_link, inward, outward)\n",
        "        GraphObj = Graph(**graph_args)\n",
        "        A = GraphObj.A\n",
        "        self.data_bn = nn.BatchNorm1d(num_person * in_channels * num_point)\n",
        "        self.l1 = TCN_GCN_unit(3, 64, A, residual=False)\n",
        "        self.l2 = TCN_GCN_unit(64, 64, A)\n",
        "        self.l3 = TCN_GCN_unit(64, 64, A)\n",
        "        self.l4 = TCN_GCN_unit(64, 64, A)\n",
        "        self.l5 = TCN_GCN_unit(64, 128, A, stride=2)\n",
        "        self.l6 = TCN_GCN_unit(128, 128, A)\n",
        "        self.l7 = TCN_GCN_unit(128, 128, A)\n",
        "        self.l8 = TCN_GCN_unit(128, 256, A, stride=2)\n",
        "        self.l9 = TCN_GCN_unit(256, 256, A)\n",
        "        self.l10 = TCN_GCN_unit(256, 256, A)\n",
        "        self.dropout = nn.Dropout(0.5)  # Enhanced regularization\n",
        "        self.fc = nn.Linear(256, num_class)\n",
        "        nn.init.normal_(self.fc.weight, 0, math.sqrt(2. / num_class))\n",
        "        bn_init(self.data_bn, 1)\n",
        "    def forward(self, x):\n",
        "        N, C, T, V, M = x.size()\n",
        "        x = x.permute(0, 4, 3, 1, 2).contiguous().view(N, M * V * C, T)\n",
        "        x = self.data_bn(x)\n",
        "        x = x.view(N, M, V, C, T).permute(0, 1, 3, 4, 2).contiguous().view(N * M, C, T, V)\n",
        "        x = self.l1(x)\n",
        "        x = self.l2(x)\n",
        "        x = self.l3(x)\n",
        "        x = self.l4(x)\n",
        "        x = self.l5(x)\n",
        "        x = self.l6(x)\n",
        "        x = self.l7(x)\n",
        "        x = self.l8(x)\n",
        "        x = self.l9(x)\n",
        "        x = self.l10(x)\n",
        "        c_new = x.size(1)\n",
        "        x = x.view(N, M, c_new, -1)\n",
        "        x = x.mean(3).mean(1)\n",
        "        x = self.dropout(x)  # Apply dropout\n",
        "        return self.fc(x)"
      ],
      "metadata": {
        "id": "ndUXSYHGVVb6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Training Function with Optimizations**"
      ],
      "metadata": {
        "id": "k76p7AB0ViHH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_evaluate_agcn(\n",
        "    train_data_path, train_label_path,\n",
        "    val_data_path, val_label_path,\n",
        "    save_path, batch_size=64, epochs=90, patience=35, plot_prefix=None\n",
        "):\n",
        "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # Initialize datasets with enhanced augmentation\n",
        "    train_set = Feeder(train_data_path, train_label_path, train=True)\n",
        "    val_set = Feeder(val_data_path, val_label_path)\n",
        "\n",
        "    def custom_collate(batch):\n",
        "        data = [item[0] for item in batch]\n",
        "        labels = [item[1] for item in batch]\n",
        "        data = torch.stack([torch.from_numpy(np.copy(d)).float() for d in data], 0)\n",
        "        labels = torch.LongTensor(labels)\n",
        "        return data, labels\n",
        "\n",
        "    train_loader = DataLoader(train_set, batch_size=batch_size,\n",
        "                            shuffle=True, num_workers=4,\n",
        "                            pin_memory=True, collate_fn=custom_collate)\n",
        "    val_loader = DataLoader(val_set, batch_size=batch_size,\n",
        "                          shuffle=False, num_workers=2,\n",
        "                          pin_memory=True, collate_fn=custom_collate)\n",
        "\n",
        "    model = Model(num_class=60, num_point=25, num_person=2).to(DEVICE)\n",
        "\n",
        "    # Enhanced optimizer configuration\n",
        "    optimizer = optim.SGD(\n",
        "        model.parameters(),\n",
        "        lr=0.1,  # Will be overridden by warmup\n",
        "        momentum=0.95,\n",
        "        nesterov=True,\n",
        "        weight_decay=0.0002\n",
        "    )\n",
        "\n",
        "    # Learning rate scheduler with warmup\n",
        "    def adjust_learning_rate(optimizer, epoch):\n",
        "        if epoch < 15:  # Warmup phase\n",
        "            lr = 0.1 * (epoch + 1) / 15\n",
        "        else:  # Follow original schedule\n",
        "            if epoch >= 30 and epoch < 50:\n",
        "                lr = 0.02\n",
        "            elif epoch >= 50:\n",
        "                lr = 0.004\n",
        "            else:\n",
        "                lr = 0.1\n",
        "\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group['lr'] = lr\n",
        "        return lr\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "    scaler = amp.GradScaler()\n",
        "\n",
        "    best_acc = 0.0\n",
        "    best_epoch = 0\n",
        "    patience_counter = 0\n",
        "    history = {\n",
        "        'train_loss': [],\n",
        "        'train_acc': [],\n",
        "        'val_loss': [],\n",
        "        'val_acc': [],\n",
        "        'val_confidence': [],\n",
        "        'lr': []\n",
        "    }\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        torch.save({'epoch': epoch}, '/tmp/current_epoch.pth')  # For augmentation\n",
        "        start_time = time.time()\n",
        "        model.train()\n",
        "        running_loss, total, correct = 0.0, 0, 0\n",
        "\n",
        "        current_lr = adjust_learning_rate(optimizer, epoch)\n",
        "\n",
        "        for data, label in train_loader:\n",
        "            data, label = data.to(DEVICE), label.to(DEVICE)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            with amp.autocast():\n",
        "                outputs = model(data)\n",
        "                loss = criterion(outputs, label)\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "\n",
        "            # Gradient clipping with AMP compatibility\n",
        "            scaler.unscale_(optimizer)\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=2.0)\n",
        "\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "            running_loss += loss.item() * data.size(0)\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += label.size(0)\n",
        "            correct += predicted.eq(label).sum().item()\n",
        "\n",
        "        train_loss = running_loss / total\n",
        "        train_acc = correct / total\n",
        "\n",
        "        # Enhanced validation with confidence monitoring\n",
        "        model.eval()\n",
        "        val_loss, val_total, val_correct = 0.0, 0, 0\n",
        "        confidences = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for data, label in val_loader:\n",
        "                data, label = data.to(DEVICE), label.to(DEVICE)\n",
        "                outputs = model(data)\n",
        "                loss = criterion(outputs, label)\n",
        "\n",
        "                # Calculate prediction confidence\n",
        "                probs = torch.softmax(outputs, dim=1)\n",
        "                top2_probs, _ = probs.topk(2, dim=1)\n",
        "                batch_confidences = (top2_probs[:,0] - top2_probs[:,1]).cpu().numpy()\n",
        "                confidences.extend(batch_confidences)\n",
        "\n",
        "                val_loss += loss.item() * data.size(0)\n",
        "                _, predicted = outputs.max(1)\n",
        "                val_total += label.size(0)\n",
        "                val_correct += predicted.eq(label).sum().item()\n",
        "\n",
        "        val_loss /= val_total\n",
        "        val_acc = val_correct / val_total\n",
        "        val_confidence = np.mean(confidences) * 100  # Convert to percentage\n",
        "\n",
        "        # Update history\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['train_acc'].append(train_acc)\n",
        "        history['val_loss'].append(val_loss)\n",
        "        history['val_acc'].append(val_acc)\n",
        "        history['val_confidence'].append(val_confidence)\n",
        "        history['lr'].append(current_lr)\n",
        "\n",
        "        epoch_time = time.time() - start_time\n",
        "\n",
        "        # Enhanced logging\n",
        "        print(f\"Epoch {epoch+1:2d}/{epochs} | Time: {epoch_time:.1f}s | \"\n",
        "              f\"LR: {current_lr:.6f} | \"\n",
        "              f\"Train Loss: {train_loss:.4f} Acc: {train_acc:.4f} | \"\n",
        "              f\"Val Loss: {val_loss:.4f} Acc: {val_acc:.4f} | \"\n",
        "              f\"Confidence: {val_confidence:.2f}%\")\n",
        "\n",
        "        # Save best model\n",
        "        if val_acc > best_acc:\n",
        "            best_acc = val_acc\n",
        "            best_epoch = epoch + 1\n",
        "            torch.save(model.state_dict(), save_path)\n",
        "            patience_counter = 0\n",
        "            print(f\"  New best model saved! (Acc: {best_acc:.4f})\")\n",
        "\n",
        "            # Early exit if target achieved\n",
        "            if val_acc >= 0.91 and \"xsub\" in save_path.lower():\n",
        "                print(\"  X-Sub target accuracy achieved!\")\n",
        "                break\n",
        "            if val_acc >= 0.93 and \"xview\" in save_path.lower():\n",
        "                print(\"  X-View target accuracy achieved!\")\n",
        "                break\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print(f\"Early stopping at epoch {epoch+1}\")\n",
        "                break\n",
        "\n",
        "    print(f\"Best Validation Accuracy: {best_acc:.4f} at Epoch {best_epoch}\")\n",
        "\n",
        "    # Plot final results\n",
        "    if plot_prefix:\n",
        "        plt.figure(figsize=(15,5))\n",
        "\n",
        "        # Accuracy plot\n",
        "        plt.subplot(1,3,1)\n",
        "        plt.plot(history['train_acc'], label='Train')\n",
        "        plt.plot(history['val_acc'], label='Val')\n",
        "        plt.axhline(0.91, c='r', linestyle='--', label='Target')\n",
        "        plt.title('Accuracy')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.legend()\n",
        "\n",
        "        # Loss plot\n",
        "        plt.subplot(1,3,2)\n",
        "        plt.plot(history['train_loss'], label='Train')\n",
        "        plt.plot(history['val_loss'], label='Val')\n",
        "        plt.title('Loss')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.legend()\n",
        "\n",
        "        # Confidence plot\n",
        "        plt.subplot(1,3,3)\n",
        "        plt.plot(history['val_confidence'], label='Val Confidence')\n",
        "        plt.axhline(30, c='g', linestyle='--', label='Good')\n",
        "        plt.axhline(50, c='b', linestyle='--', label='Excellent')\n",
        "        plt.title('Validation Confidence (%)')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.legend()\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f\"{plot_prefix}_training_curves.png\")\n",
        "        plt.show()\n",
        "\n",
        "    return best_acc, best_epoch, history"
      ],
      "metadata": {
        "id": "x54sZiKZVgPi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Run Training (X-Sub)**"
      ],
      "metadata": {
        "id": "u4EqyicDV9Ae"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6: Run Training (X-Sub)\n",
        "print(\"Starting AGCN training for X-Sub...\")\n",
        "xsub_best_acc, xsub_best_epoch, xsub_history = train_and_evaluate_agcn(\n",
        "    train_data, train_label,\n",
        "    val_data, val_label,\n",
        "    save_path='/content/drive/MyDrive/agcn_xsub_enhanced--.pth',\n",
        "    batch_size=64,  # Official batch size\n",
        "    epochs=90,\n",
        "    patience=35,\n",
        "    plot_prefix='AGCN_XSUB'\n",
        ")\n",
        "print(f\"\\nX-Sub Final Best Validation Accuracy: {xsub_best_acc:.4f} at Epoch {xsub_best_epoch}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "v8nIpjBTV9eb",
        "outputId": "2f3b7396-5e77-42ad-c2d8-3867f36ecf7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting AGCN training for X-Sub...\n",
            "Epoch  1/90 | Time: 301.4s | Train Loss: 3.9525 Acc: 0.0394 | Val Loss: 3.6345 Acc: 0.0709 | LR: 0.100000\n",
            "  New best model saved! (Acc: 0.0709)\n",
            "Epoch  2/90 | Time: 272.6s | Train Loss: 3.3253 Acc: 0.0963 | Val Loss: 3.0727 Acc: 0.1465 | LR: 0.100000\n",
            "  New best model saved! (Acc: 0.1465)\n",
            "Epoch  3/90 | Time: 272.3s | Train Loss: 2.8582 Acc: 0.1767 | Val Loss: 2.7668 Acc: 0.2113 | LR: 0.100000\n",
            "  New best model saved! (Acc: 0.2113)\n",
            "Epoch  4/90 | Time: 272.2s | Train Loss: 2.4878 Acc: 0.2600 | Val Loss: 2.8777 Acc: 0.2110 | LR: 0.100000\n",
            "Epoch  5/90 | Time: 272.0s | Train Loss: 2.1237 Acc: 0.3622 | Val Loss: 2.3933 Acc: 0.3119 | LR: 0.100000\n",
            "  New best model saved! (Acc: 0.3119)\n",
            "Epoch  6/90 | Time: 272.2s | Train Loss: 1.8427 Acc: 0.4435 | Val Loss: 2.0344 Acc: 0.3903 | LR: 0.100000\n",
            "  New best model saved! (Acc: 0.3903)\n",
            "Epoch  7/90 | Time: 272.0s | Train Loss: 1.5982 Acc: 0.5210 | Val Loss: 1.7859 Acc: 0.4921 | LR: 0.100000\n",
            "  New best model saved! (Acc: 0.4921)\n",
            "Epoch  8/90 | Time: 271.8s | Train Loss: 1.4064 Acc: 0.5789 | Val Loss: 1.6238 Acc: 0.5201 | LR: 0.100000\n",
            "  New best model saved! (Acc: 0.5201)\n",
            "Epoch  9/90 | Time: 272.3s | Train Loss: 1.2729 Acc: 0.6146 | Val Loss: 1.6192 Acc: 0.5305 | LR: 0.100000\n",
            "  New best model saved! (Acc: 0.5305)\n",
            "Epoch 10/90 | Time: 272.1s | Train Loss: 1.1645 Acc: 0.6502 | Val Loss: 1.2855 Acc: 0.6101 | LR: 0.100000\n",
            "  New best model saved! (Acc: 0.6101)\n",
            "Epoch 11/90 | Time: 272.5s | Train Loss: 1.0851 Acc: 0.6724 | Val Loss: 1.2617 Acc: 0.6185 | LR: 0.100000\n",
            "  New best model saved! (Acc: 0.6185)\n",
            "Epoch 12/90 | Time: 271.9s | Train Loss: 1.0196 Acc: 0.6915 | Val Loss: 1.3075 Acc: 0.6137 | LR: 0.100000\n",
            "Epoch 13/90 | Time: 272.0s | Train Loss: 0.9581 Acc: 0.7091 | Val Loss: 1.1885 Acc: 0.6508 | LR: 0.100000\n",
            "  New best model saved! (Acc: 0.6508)\n",
            "Epoch 14/90 | Time: 271.6s | Train Loss: 0.9063 Acc: 0.7234 | Val Loss: 1.1893 Acc: 0.6453 | LR: 0.100000\n",
            "Epoch 15/90 | Time: 271.8s | Train Loss: 0.8660 Acc: 0.7349 | Val Loss: 1.2326 Acc: 0.6339 | LR: 0.100000\n",
            "Epoch 16/90 | Time: 271.7s | Train Loss: 0.8335 Acc: 0.7431 | Val Loss: 1.1066 Acc: 0.6669 | LR: 0.100000\n",
            "  New best model saved! (Acc: 0.6669)\n",
            "Epoch 17/90 | Time: 271.6s | Train Loss: 0.8023 Acc: 0.7526 | Val Loss: 1.0147 Acc: 0.6934 | LR: 0.100000\n",
            "  New best model saved! (Acc: 0.6934)\n",
            "Epoch 18/90 | Time: 271.5s | Train Loss: 0.7665 Acc: 0.7653 | Val Loss: 1.0299 Acc: 0.6955 | LR: 0.100000\n",
            "  New best model saved! (Acc: 0.6955)\n",
            "Epoch 19/90 | Time: 271.6s | Train Loss: 0.7351 Acc: 0.7723 | Val Loss: 1.0658 Acc: 0.6836 | LR: 0.100000\n",
            "Epoch 20/90 | Time: 271.6s | Train Loss: 0.7128 Acc: 0.7774 | Val Loss: 1.1152 Acc: 0.6729 | LR: 0.100000\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-17-3872708398.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Cell 6: Run Training (X-Sub)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Starting AGCN training for X-Sub...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m xsub_best_acc, xsub_best_epoch, xsub_history = train_and_evaluate_agcn(\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_label\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mval_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_label\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-16-2720743652.py\u001b[0m in \u001b[0;36mtrain_and_evaluate_agcn\u001b[0;34m(train_data_path, train_label_path, val_data_path, val_label_path, save_path, batch_size, epochs, patience, plot_prefix)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# Use the updated way to call autocast\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-9-547280227.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml7\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml8\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml9\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml10\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0mc_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-9-547280227.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munit_tcn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_channels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_channels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtcn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgcn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresidual\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-9-547280227.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_subset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0mA1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_a\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minter_c\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mA2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_b\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minter_c\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mA1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1e-8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 554\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    555\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    547\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m             )\n\u001b[0;32m--> 549\u001b[0;31m         return F.conv2d(\n\u001b[0m\u001b[1;32m    550\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m         )\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1d486480",
        "outputId": "9e043a20-13ed-4405-f2ff-aafebb6c8cb1"
      },
      "source": [
        "# Initialize the model before training\n",
        "model = Model(num_class=60, num_point=25, num_person=2).to(DEVICE)\n",
        "print(\"Model initialized.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model initialized.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "\n",
        "# Filter out the specific FutureWarning related to GradScaler\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning, message=\"`torch.cuda.amp.GradScaler(args...)` is deprecated\")\n",
        "# Also filter out FutureWarnings from the torch.cuda.amp module more broadly\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"torch.cuda.amp\")\n",
        "# Add a general filter for all FutureWarnings as a fallback\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n"
      ],
      "metadata": {
        "id": "USHGgIgMtObY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add this before starting/resuming training\n",
        "if os.path.exists('/content/drive/MyDrive/agcn_xsub_enhanced--.pth'):\n",
        "    model.load_state_dict(torch.load('/content/drive/MyDrive/agcn_xsub_enhanced--.pth'))\n",
        "    print(\"Loaded existing model weights\")\n",
        "\n",
        "# Run training with all enhancements\n",
        "print(\"Starting enhanced training...\")\n",
        "best_acc, best_epoch, history = train_and_evaluate_agcn(\n",
        "    train_data, train_label,\n",
        "    val_data, val_label,\n",
        "    save_path='/content/drive/MyDrive/agcn_xsub_enhanced--.pth',\n",
        "    batch_size=64,\n",
        "    epochs=90,\n",
        "    patience=35\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xZN2z04Fs9QY",
        "outputId": "bd0a1238-5720-4525-ec4a-8529072ffc26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded existing model weights\n",
            "Starting enhanced training...\n",
            "Epoch  1/90 | Time: 272.6s | LR: 0.006667 | Train Loss: 4.0678 Acc: 0.0634 | Val Loss: 3.2748 Acc: 0.1659 | Confidence: 4.78%\n",
            "  New best model saved! (Acc: 0.1659)\n",
            "Epoch  2/90 | Time: 273.6s | LR: 0.013333 | Train Loss: 3.0681 Acc: 0.2138 | Val Loss: 2.5966 Acc: 0.3734 | Confidence: 11.87%\n",
            "  New best model saved! (Acc: 0.3734)\n",
            "Epoch  3/90 | Time: 272.6s | LR: 0.020000 | Train Loss: 2.5558 Acc: 0.3753 | Val Loss: 2.2018 Acc: 0.4986 | Confidence: 20.76%\n",
            "  New best model saved! (Acc: 0.4986)\n",
            "Epoch  4/90 | Time: 272.6s | LR: 0.026667 | Train Loss: 2.2417 Acc: 0.4903 | Val Loss: 1.9000 Acc: 0.6094 | Confidence: 33.04%\n",
            "  New best model saved! (Acc: 0.6094)\n",
            "Epoch  5/90 | Time: 272.5s | LR: 0.033333 | Train Loss: 2.0242 Acc: 0.5710 | Val Loss: 1.8065 Acc: 0.6433 | Confidence: 38.81%\n",
            "  New best model saved! (Acc: 0.6433)\n",
            "Epoch  6/90 | Time: 272.2s | LR: 0.040000 | Train Loss: 1.8790 Acc: 0.6241 | Val Loss: 1.6984 Acc: 0.6827 | Confidence: 42.86%\n",
            "  New best model saved! (Acc: 0.6827)\n",
            "Epoch  7/90 | Time: 272.5s | LR: 0.046667 | Train Loss: 1.7991 Acc: 0.6524 | Val Loss: 1.6754 Acc: 0.6901 | Confidence: 47.34%\n",
            "  New best model saved! (Acc: 0.6901)\n",
            "Epoch  8/90 | Time: 272.4s | LR: 0.053333 | Train Loss: 1.7080 Acc: 0.6883 | Val Loss: 1.5789 Acc: 0.7292 | Confidence: 48.83%\n",
            "  New best model saved! (Acc: 0.7292)\n",
            "Epoch  9/90 | Time: 272.2s | LR: 0.060000 | Train Loss: 1.6602 Acc: 0.7050 | Val Loss: 1.6063 Acc: 0.7161 | Confidence: 49.34%\n",
            "Epoch 10/90 | Time: 272.5s | LR: 0.066667 | Train Loss: 1.6223 Acc: 0.7177 | Val Loss: 1.5748 Acc: 0.7172 | Confidence: 51.81%\n",
            "Epoch 11/90 | Time: 272.2s | LR: 0.073333 | Train Loss: 1.5829 Acc: 0.7332 | Val Loss: 1.4791 Acc: 0.7642 | Confidence: 57.98%\n",
            "  New best model saved! (Acc: 0.7642)\n",
            "Epoch 12/90 | Time: 272.3s | LR: 0.080000 | Train Loss: 1.5708 Acc: 0.7376 | Val Loss: 1.4973 Acc: 0.7515 | Confidence: 54.52%\n",
            "Epoch 13/90 | Time: 272.3s | LR: 0.086667 | Train Loss: 1.5567 Acc: 0.7430 | Val Loss: 1.4811 Acc: 0.7586 | Confidence: 56.07%\n",
            "Epoch 14/90 | Time: 272.4s | LR: 0.093333 | Train Loss: 1.5372 Acc: 0.7534 | Val Loss: 1.4940 Acc: 0.7572 | Confidence: 55.74%\n",
            "Epoch 15/90 | Time: 272.3s | LR: 0.100000 | Train Loss: 1.5410 Acc: 0.7497 | Val Loss: 1.5567 Acc: 0.7348 | Confidence: 56.32%\n",
            "Epoch 16/90 | Time: 272.6s | LR: 0.100000 | Train Loss: 1.5082 Acc: 0.7625 | Val Loss: 1.4846 Acc: 0.7554 | Confidence: 58.00%\n",
            "Epoch 17/90 | Time: 272.3s | LR: 0.100000 | Train Loss: 1.4988 Acc: 0.7661 | Val Loss: 1.4441 Acc: 0.7717 | Confidence: 60.83%\n",
            "  New best model saved! (Acc: 0.7717)\n",
            "Epoch 18/90 | Time: 272.3s | LR: 0.100000 | Train Loss: 1.4794 Acc: 0.7724 | Val Loss: 1.4656 Acc: 0.7620 | Confidence: 58.45%\n",
            "Epoch 19/90 | Time: 272.4s | LR: 0.100000 | Train Loss: 1.4579 Acc: 0.7797 | Val Loss: 1.4421 Acc: 0.7706 | Confidence: 56.47%\n",
            "Epoch 20/90 | Time: 272.4s | LR: 0.100000 | Train Loss: 1.4474 Acc: 0.7818 | Val Loss: 1.4030 Acc: 0.7848 | Confidence: 60.70%\n",
            "  New best model saved! (Acc: 0.7848)\n",
            "Epoch 21/90 | Time: 272.5s | LR: 0.100000 | Train Loss: 1.4363 Acc: 0.7852 | Val Loss: 1.4363 Acc: 0.7696 | Confidence: 57.91%\n",
            "Epoch 22/90 | Time: 272.1s | LR: 0.100000 | Train Loss: 1.4303 Acc: 0.7880 | Val Loss: 1.4551 Acc: 0.7622 | Confidence: 59.40%\n",
            "Epoch 23/90 | Time: 272.3s | LR: 0.100000 | Train Loss: 1.4199 Acc: 0.7923 | Val Loss: 1.4272 Acc: 0.7716 | Confidence: 59.97%\n",
            "Epoch 24/90 | Time: 272.3s | LR: 0.100000 | Train Loss: 1.4163 Acc: 0.7913 | Val Loss: 1.4170 Acc: 0.7839 | Confidence: 64.33%\n",
            "Epoch 25/90 | Time: 271.9s | LR: 0.100000 | Train Loss: 1.4009 Acc: 0.7965 | Val Loss: 1.3834 Acc: 0.7958 | Confidence: 61.53%\n",
            "  New best model saved! (Acc: 0.7958)\n",
            "Epoch 26/90 | Time: 272.5s | LR: 0.100000 | Train Loss: 1.4081 Acc: 0.7948 | Val Loss: 1.4702 Acc: 0.7667 | Confidence: 60.52%\n",
            "Epoch 27/90 | Time: 272.4s | LR: 0.100000 | Train Loss: 1.3970 Acc: 0.7984 | Val Loss: 1.4499 Acc: 0.7635 | Confidence: 60.90%\n",
            "Epoch 28/90 | Time: 272.4s | LR: 0.100000 | Train Loss: 1.3857 Acc: 0.8027 | Val Loss: 1.3595 Acc: 0.8016 | Confidence: 64.19%\n",
            "  New best model saved! (Acc: 0.8016)\n",
            "Epoch 29/90 | Time: 272.1s | LR: 0.100000 | Train Loss: 1.3809 Acc: 0.8042 | Val Loss: 1.3635 Acc: 0.7995 | Confidence: 62.21%\n",
            "Epoch 30/90 | Time: 272.4s | LR: 0.100000 | Train Loss: 1.3781 Acc: 0.8032 | Val Loss: 1.3718 Acc: 0.7953 | Confidence: 62.44%\n",
            "Epoch 31/90 | Time: 272.3s | LR: 0.020000 | Train Loss: 1.1572 Acc: 0.8820 | Val Loss: 1.1526 Acc: 0.8637 | Confidence: 71.42%\n",
            "  New best model saved! (Acc: 0.8637)\n",
            "Epoch 32/90 | Time: 272.1s | LR: 0.020000 | Train Loss: 1.0756 Acc: 0.9089 | Val Loss: 1.1486 Acc: 0.8665 | Confidence: 73.63%\n",
            "  New best model saved! (Acc: 0.8665)\n",
            "Epoch 33/90 | Time: 272.9s | LR: 0.020000 | Train Loss: 1.0454 Acc: 0.9201 | Val Loss: 1.1459 Acc: 0.8651 | Confidence: 73.47%\n",
            "Epoch 34/90 | Time: 272.5s | LR: 0.020000 | Train Loss: 1.0238 Acc: 0.9274 | Val Loss: 1.1488 Acc: 0.8632 | Confidence: 73.15%\n",
            "Epoch 35/90 | Time: 272.9s | LR: 0.020000 | Train Loss: 1.0071 Acc: 0.9350 | Val Loss: 1.1380 Acc: 0.8676 | Confidence: 73.85%\n",
            "  New best model saved! (Acc: 0.8676)\n",
            "Epoch 36/90 | Time: 273.0s | LR: 0.020000 | Train Loss: 0.9930 Acc: 0.9383 | Val Loss: 1.1601 Acc: 0.8594 | Confidence: 72.29%\n",
            "Epoch 37/90 | Time: 272.0s | LR: 0.020000 | Train Loss: 0.9827 Acc: 0.9437 | Val Loss: 1.1722 Acc: 0.8584 | Confidence: 73.58%\n",
            "Epoch 38/90 | Time: 272.2s | LR: 0.020000 | Train Loss: 0.9777 Acc: 0.9435 | Val Loss: 1.1692 Acc: 0.8602 | Confidence: 73.94%\n",
            "Epoch 39/90 | Time: 272.7s | LR: 0.020000 | Train Loss: 0.9687 Acc: 0.9481 | Val Loss: 1.1601 Acc: 0.8628 | Confidence: 73.89%\n",
            "Epoch 40/90 | Time: 272.6s | LR: 0.020000 | Train Loss: 0.9648 Acc: 0.9486 | Val Loss: 1.1765 Acc: 0.8612 | Confidence: 74.35%\n",
            "Epoch 41/90 | Time: 272.7s | LR: 0.020000 | Train Loss: 0.9576 Acc: 0.9520 | Val Loss: 1.1822 Acc: 0.8567 | Confidence: 73.40%\n",
            "Epoch 42/90 | Time: 272.8s | LR: 0.020000 | Train Loss: 0.9555 Acc: 0.9513 | Val Loss: 1.1916 Acc: 0.8537 | Confidence: 74.25%\n",
            "Epoch 43/90 | Time: 272.5s | LR: 0.020000 | Train Loss: 0.9529 Acc: 0.9526 | Val Loss: 1.1901 Acc: 0.8544 | Confidence: 74.00%\n",
            "Epoch 44/90 | Time: 272.4s | LR: 0.020000 | Train Loss: 0.9543 Acc: 0.9520 | Val Loss: 1.2188 Acc: 0.8475 | Confidence: 73.97%\n",
            "Epoch 45/90 | Time: 272.5s | LR: 0.020000 | Train Loss: 0.9509 Acc: 0.9529 | Val Loss: 1.2132 Acc: 0.8510 | Confidence: 73.67%\n",
            "Epoch 46/90 | Time: 272.1s | LR: 0.020000 | Train Loss: 0.9511 Acc: 0.9544 | Val Loss: 1.2027 Acc: 0.8513 | Confidence: 74.53%\n",
            "Epoch 47/90 | Time: 272.2s | LR: 0.020000 | Train Loss: 0.9420 Acc: 0.9571 | Val Loss: 1.2341 Acc: 0.8437 | Confidence: 73.07%\n",
            "Epoch 48/90 | Time: 272.5s | LR: 0.020000 | Train Loss: 0.9412 Acc: 0.9564 | Val Loss: 1.1996 Acc: 0.8530 | Confidence: 75.12%\n",
            "Epoch 49/90 | Time: 272.2s | LR: 0.020000 | Train Loss: 0.9362 Acc: 0.9593 | Val Loss: 1.2237 Acc: 0.8442 | Confidence: 73.52%\n",
            "Epoch 50/90 | Time: 272.2s | LR: 0.020000 | Train Loss: 0.9288 Acc: 0.9623 | Val Loss: 1.2335 Acc: 0.8460 | Confidence: 74.73%\n",
            "Epoch 51/90 | Time: 272.1s | LR: 0.004000 | Train Loss: 0.8484 Acc: 0.9879 | Val Loss: 1.1166 Acc: 0.8741 | Confidence: 76.23%\n",
            "  New best model saved! (Acc: 0.8741)\n",
            "Epoch 52/90 | Time: 272.3s | LR: 0.004000 | Train Loss: 0.8206 Acc: 0.9947 | Val Loss: 1.1143 Acc: 0.8771 | Confidence: 76.29%\n",
            "  New best model saved! (Acc: 0.8771)\n",
            "Epoch 53/90 | Time: 272.2s | LR: 0.004000 | Train Loss: 0.8122 Acc: 0.9966 | Val Loss: 1.1133 Acc: 0.8792 | Confidence: 75.99%\n",
            "  New best model saved! (Acc: 0.8792)\n",
            "Epoch 54/90 | Time: 272.3s | LR: 0.004000 | Train Loss: 0.8060 Acc: 0.9977 | Val Loss: 1.1164 Acc: 0.8772 | Confidence: 76.05%\n",
            "Epoch 55/90 | Time: 272.6s | LR: 0.004000 | Train Loss: 0.8025 Acc: 0.9980 | Val Loss: 1.1167 Acc: 0.8781 | Confidence: 75.80%\n",
            "Epoch 56/90 | Time: 272.1s | LR: 0.004000 | Train Loss: 0.8002 Acc: 0.9984 | Val Loss: 1.1136 Acc: 0.8805 | Confidence: 75.93%\n",
            "  New best model saved! (Acc: 0.8805)\n",
            "Epoch 57/90 | Time: 272.0s | LR: 0.004000 | Train Loss: 0.7972 Acc: 0.9988 | Val Loss: 1.1148 Acc: 0.8795 | Confidence: 76.05%\n",
            "Epoch 58/90 | Time: 272.2s | LR: 0.004000 | Train Loss: 0.7950 Acc: 0.9990 | Val Loss: 1.1198 Acc: 0.8792 | Confidence: 75.35%\n",
            "Epoch 59/90 | Time: 272.5s | LR: 0.004000 | Train Loss: 0.7933 Acc: 0.9992 | Val Loss: 1.1181 Acc: 0.8804 | Confidence: 75.41%\n",
            "Epoch 60/90 | Time: 272.7s | LR: 0.004000 | Train Loss: 0.7923 Acc: 0.9989 | Val Loss: 1.1208 Acc: 0.8779 | Confidence: 75.59%\n",
            "Epoch 61/90 | Time: 272.4s | LR: 0.004000 | Train Loss: 0.7918 Acc: 0.9991 | Val Loss: 1.1207 Acc: 0.8806 | Confidence: 75.46%\n",
            "  New best model saved! (Acc: 0.8806)\n",
            "Epoch 62/90 | Time: 272.3s | LR: 0.004000 | Train Loss: 0.7899 Acc: 0.9995 | Val Loss: 1.1198 Acc: 0.8790 | Confidence: 76.07%\n",
            "Epoch 63/90 | Time: 272.5s | LR: 0.004000 | Train Loss: 0.7887 Acc: 0.9996 | Val Loss: 1.1202 Acc: 0.8801 | Confidence: 75.01%\n",
            "Epoch 64/90 | Time: 272.3s | LR: 0.004000 | Train Loss: 0.7882 Acc: 0.9996 | Val Loss: 1.1199 Acc: 0.8814 | Confidence: 74.95%\n",
            "  New best model saved! (Acc: 0.8814)\n",
            "Epoch 65/90 | Time: 272.2s | LR: 0.004000 | Train Loss: 0.7866 Acc: 0.9998 | Val Loss: 1.1222 Acc: 0.8801 | Confidence: 75.24%\n",
            "Epoch 66/90 | Time: 272.6s | LR: 0.004000 | Train Loss: 0.7859 Acc: 0.9997 | Val Loss: 1.1258 Acc: 0.8793 | Confidence: 75.04%\n",
            "Epoch 67/90 | Time: 272.4s | LR: 0.004000 | Train Loss: 0.7854 Acc: 0.9995 | Val Loss: 1.1265 Acc: 0.8798 | Confidence: 75.15%\n",
            "Epoch 68/90 | Time: 272.4s | LR: 0.004000 | Train Loss: 0.7849 Acc: 0.9998 | Val Loss: 1.1258 Acc: 0.8786 | Confidence: 75.02%\n",
            "Epoch 69/90 | Time: 272.2s | LR: 0.004000 | Train Loss: 0.7845 Acc: 0.9998 | Val Loss: 1.1246 Acc: 0.8780 | Confidence: 75.56%\n",
            "Epoch 70/90 | Time: 272.8s | LR: 0.004000 | Train Loss: 0.7836 Acc: 0.9999 | Val Loss: 1.1269 Acc: 0.8790 | Confidence: 75.01%\n",
            "Epoch 71/90 | Time: 272.2s | LR: 0.004000 | Train Loss: 0.7828 Acc: 0.9999 | Val Loss: 1.1279 Acc: 0.8795 | Confidence: 74.67%\n",
            "Epoch 72/90 | Time: 272.3s | LR: 0.004000 | Train Loss: 0.7829 Acc: 0.9998 | Val Loss: 1.1294 Acc: 0.8787 | Confidence: 75.29%\n",
            "Epoch 73/90 | Time: 271.9s | LR: 0.004000 | Train Loss: 0.7825 Acc: 0.9996 | Val Loss: 1.1295 Acc: 0.8793 | Confidence: 74.81%\n",
            "Epoch 74/90 | Time: 272.4s | LR: 0.004000 | Train Loss: 0.7821 Acc: 0.9999 | Val Loss: 1.1268 Acc: 0.8805 | Confidence: 75.14%\n",
            "Epoch 75/90 | Time: 272.4s | LR: 0.004000 | Train Loss: 0.7814 Acc: 0.9999 | Val Loss: 1.1312 Acc: 0.8802 | Confidence: 74.49%\n",
            "Epoch 76/90 | Time: 272.2s | LR: 0.004000 | Train Loss: 0.7807 Acc: 1.0000 | Val Loss: 1.1247 Acc: 0.8801 | Confidence: 75.10%\n",
            "Epoch 77/90 | Time: 272.5s | LR: 0.004000 | Train Loss: 0.7808 Acc: 0.9999 | Val Loss: 1.1325 Acc: 0.8780 | Confidence: 74.90%\n",
            "Epoch 78/90 | Time: 272.3s | LR: 0.004000 | Train Loss: 0.7800 Acc: 0.9999 | Val Loss: 1.1344 Acc: 0.8766 | Confidence: 74.50%\n",
            "Epoch 79/90 | Time: 272.4s | LR: 0.004000 | Train Loss: 0.7800 Acc: 0.9999 | Val Loss: 1.1316 Acc: 0.8788 | Confidence: 74.61%\n",
            "Epoch 80/90 | Time: 272.3s | LR: 0.004000 | Train Loss: 0.7803 Acc: 0.9997 | Val Loss: 1.1326 Acc: 0.8794 | Confidence: 74.71%\n",
            "Epoch 81/90 | Time: 272.3s | LR: 0.004000 | Train Loss: 0.7797 Acc: 0.9998 | Val Loss: 1.1337 Acc: 0.8791 | Confidence: 74.75%\n",
            "Epoch 82/90 | Time: 272.2s | LR: 0.004000 | Train Loss: 0.7783 Acc: 0.9999 | Val Loss: 1.1344 Acc: 0.8797 | Confidence: 74.31%\n",
            "Epoch 83/90 | Time: 272.3s | LR: 0.004000 | Train Loss: 0.7789 Acc: 0.9998 | Val Loss: 1.1360 Acc: 0.8789 | Confidence: 74.82%\n",
            "Epoch 84/90 | Time: 272.4s | LR: 0.004000 | Train Loss: 0.7794 Acc: 0.9999 | Val Loss: 1.1345 Acc: 0.8795 | Confidence: 74.57%\n",
            "Epoch 85/90 | Time: 272.5s | LR: 0.004000 | Train Loss: 0.7781 Acc: 0.9999 | Val Loss: 1.1334 Acc: 0.8804 | Confidence: 75.22%\n",
            "Epoch 86/90 | Time: 272.4s | LR: 0.004000 | Train Loss: 0.7787 Acc: 0.9997 | Val Loss: 1.1345 Acc: 0.8798 | Confidence: 74.60%\n",
            "Epoch 87/90 | Time: 272.6s | LR: 0.004000 | Train Loss: 0.7785 Acc: 0.9998 | Val Loss: 1.1335 Acc: 0.8801 | Confidence: 74.13%\n",
            "Epoch 88/90 | Time: 272.2s | LR: 0.004000 | Train Loss: 0.7780 Acc: 0.9999 | Val Loss: 1.1375 Acc: 0.8783 | Confidence: 74.62%\n",
            "Epoch 89/90 | Time: 272.4s | LR: 0.004000 | Train Loss: 0.7776 Acc: 0.9998 | Val Loss: 1.1334 Acc: 0.8799 | Confidence: 75.12%\n",
            "Epoch 90/90 | Time: 272.7s | LR: 0.004000 | Train Loss: 0.7776 Acc: 1.0000 | Val Loss: 1.1401 Acc: 0.8784 | Confidence: 74.41%\n",
            "Best Validation Accuracy: 0.8814 at Epoch 64\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}